#!/usr/bin/env python
# ----------------------------------------------------------------------
# Numenta Platform for Intelligent Computing (NuPIC)
# Copyright (C) 2015, Numenta, Inc.  Unless you have purchased from
# Numenta, Inc. a separate commercial license for this software code, the
# following terms and conditions apply:
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero Public License version 3 as
# published by the Free Software Foundation.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
# See the GNU Affero Public License for more details.
#
# You should have received a copy of the GNU Affero Public License
# along with this program.  If not, see http://www.gnu.org/licenses.
#
# http://numenta.org/licenses/
# ----------------------------------------------------------------------

"""Service for deleting old metric data rows. NOTE: This may not be appropriate
for all applications, particularly those that accept custom metric data with
arbitrary timestamps that are possibly in the past or future, such as HTM-IT.
"""

import argparse
import logging
import sys
import time

import sqlalchemy as sql

from nta.utils.error_handling import logExceptions
from nta.utils.logging_support_raw import LoggingSupport
from nta.utils import sqlalchemy_utils


import htmengine
import htmengine.repository
from htmengine.repository import schema



# Maximum number of rows to delete per query for reducing the likelihood of the
# MySQL "Lock wait timeout exceeded" error
_MAX_DELETE_BATCH_SIZE = 1000



# How many seconds to sleep between garbage collection cycles
_PAUSE_INTERVAL_SEC = 3600



g_log = logging.getLogger(__name__)



def _parseArgs(args):
  """Parse command-line arguments

  :param list args: the equivalent of sys.argv[1:]

  :returns: the args object generated by ``argparse.ArgumentParser.parse_args``
    with the following attributes:
      thresholdDays: Metric data rows with timestamps older than this number of
        days will be purged.

  """
  parser = argparse.ArgumentParser(description=__doc__)

  parser.add_argument(
    "--threshold-days",
    type=int,
    required=True,
    dest="thresholdDays",
    metavar="D",
    help=("Metric data rows with timestamps older than this number of days "
          "will be purged. The metric data timestamps are assumed to be "
          "UTC."))


  args = parser.parse_args()


  if args.thresholdDays <= 0:
    parser.error("--days value must be greater than zero, but got {}".format(
      args.thresholdDays))


  return args



def purgeOldMetricDataRows(thresholdDays):
  """ Purge rows from metric data table with timestamps that are older than
  the given number of days.

  :param int thresholdDays: Metric data rows with timestamps older than this
    number of days will be purged.

  :returns: number of rows that were deleted

  """
  g_log.info("Estimating number of rows in table=%s older than numDays=%s",
             schema.metric_data, thresholdDays)

  sqlEngine = htmengine.repository.engineFactory(htmengine.APP_CONFIG)

  selectionPredicate = (
    schema.metric_data.c.timestamp <
    sql.func.date_sub(sql.func.utc_timestamp(),
                      sql.text("INTERVAL {:d} DAY".format(thresholdDays)))
  )

  estimate = _estimateNumRowsToDelete(sqlEngine, selectionPredicate)

  g_log.info("Number of candidate old metric data rows to purge: estimate=%s",
             estimate)

  if estimate == 0:
    return 0

  # NOTE: We'll be deleting in smaller batches to avoid "Lock wait timeout
  # exceeded".
  #
  # When the number of old rows is huge, if we try to delete them in a single
  # transaction, we get perpetually mired in the error "Lock wait timeout
  # exceeded; try restarting transaction". mysql/innodb provides the setting
  # `innodb_lock_wait_timeout` that may be overriden, but there isn't a good way
  # to estimate a value that guarantees success. Doing it in one transaction
  # also doesn't facilitate progress update, thus creating the perception that
  # the operation is "stuck".
  totalDeleted = 0

  while totalDeleted < estimate:
    # NOTE: we're dealing with a couple of issues here:
    #
    # 1. sqlalchemy core doesn't support LIMIT in delete statements, so we can't
    #    use delete directly with LIMIT and ORDER BY
    # 2. MySql (5.6.21) doesn't support LIMIT & IN subqueries: "This version of
    #    MySQL doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery"
    #
    # So, we're going to stick with sqlalchemy, and break the operation into two
    # queries: get the candidate uid/rowid pairs, then delete matching rows

    limit = min(_MAX_DELETE_BATCH_SIZE, estimate - totalDeleted)

    uidRowidPairs = _queryCandidateRows(sqlEngine=sqlEngine,
                                        selectionPredicate=selectionPredicate,
                                        limit=limit)

    if uidRowidPairs:
      numDeleted = _deleteRows(sqlEngine=sqlEngine, uidRowidPairs=uidRowidPairs)
    else:
      # This could happen if something else deleted rows in our range
      break

    totalDeleted += numDeleted

    g_log.info("Purged %s old metric data rows [%s of %s]", numDeleted,
               totalDeleted, estimate)


  g_log.info("Purged numRows=%s of estimated=%s old metric data rows from "
             "table=%s", totalDeleted, estimate, schema.metric_data)

  return totalDeleted



@sqlalchemy_utils.retryOnTransientErrors
def _estimateNumRowsToDelete(sqlEngine, selectionPredicate):
  """
  :param sqlalchemy.engine.Engine sqlEngine:
  :param selectionPredicate: predicate for where clause that selects the desired
    rows for purging
  """
  return sqlEngine.execute(
    sql.select([sql.func.count()])
    .where(selectionPredicate)).scalar()



@sqlalchemy_utils.retryOnTransientErrors
def _queryCandidateRows(sqlEngine, selectionPredicate, limit):
  """Query candidate uid/rowid pairss of metric data rows to delete.

  :param sqlalchemy.engine.Engine sqlEngine:
  :param selectionPredicate: predicate for where clause that selects the desired
    rows for purging
  :param int limit: max number of rows to delete

  :returns: sequence of matching uid/rowid pairs (may be empty)
  """
  # Note: we order the result set to avoid creating holes in the data
  results = sqlEngine.execute(
    sql.select([schema.metric_data.c.uid, schema.metric_data.c.rowid])
    .where(selectionPredicate)
    .order_by(schema.metric_data.c.uid.asc(),
              schema.metric_data.c.rowid.asc())
    .limit(limit)
  ).fetchall()

  return tuple((str(row[0]), row[1]) for row in results)



@sqlalchemy_utils.retryOnTransientErrors
def _deleteRows(sqlEngine, uidRowidPairs):
  """Delete metric data rows with the given uid/rowid pairs

  :param sqlalchemy.engine.Engine sqlEngine:
  :param uidRowidPairs: sequence of uid/rowid pairs of metric data rows to
    delete

  :returns: number of rows actually deleted; this may be less than requested if
    something else deleted some of the requested rows
  """
  return sqlEngine.execute(
    schema.metric_data.delete()  # pylint: disable=E1120
    .where(
      sql.tuple_(schema.metric_data.c.uid, schema.metric_data.c.rowid).in_(
        uidRowidPairs))
  ).rowcount



@logExceptions(g_log)
def main():

  try:
    try:
      args = _parseArgs(sys.argv[1:])
    except SystemExit as exc:
      if exc.code == 0:
        # Suppress exception logging when exiting due to --help
        return

      raise


    while True:
      purgeOldMetricDataRows(args.thresholdDays)

      g_log.info("Resuming in %s seconds...", _PAUSE_INTERVAL_SEC)
      time.sleep(_PAUSE_INTERVAL_SEC)
  except KeyboardInterrupt:
    # Log with exception info to help debug deadlocks
    g_log.info("Observed KeyboardInterrupt", exc_info=True)



if __name__ == "__main__":
  LoggingSupport.initService()

  main()
